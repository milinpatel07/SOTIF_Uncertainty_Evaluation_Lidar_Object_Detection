{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTIF Uncertainty Evaluation for LiDAR Object Detection\n",
    "\n",
    "**Uncertainty Evaluation to Support Safety of the Intended Functionality Analysis for Identifying Performance Insufficiencies in ML-Based LiDAR Object Detection**\n",
    "\n",
    "*Milin Patel and Rolf Jung, Kempten University of Applied Sciences*\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the complete evaluation methodology from the paper. It implements all five stages of the pipeline and produces the four SOTIF artefacts mapped to ISO 21448 clauses.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Generates synthetic ensemble outputs matching the paper's statistical properties (or loads real data)\n",
    "2. Computes three uncertainty indicators per detection (Stage 2)\n",
    "3. Determines correctness labels (Stage 3)\n",
    "4. Computes discrimination, calibration, and operating characteristics (Stage 4)\n",
    "5. Produces SOTIF analysis artefacts: TC ranking, frame flags, acceptance gates (Stage 5)\n",
    "6. Generates all figures from the paper\n",
    "\n",
    "**No GPU required.** The evaluation pipeline runs on CPU. For training ensemble models on real data, see the `scripts/` directory.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection/blob/main/notebooks/SOTIF_Uncertainty_Evaluation_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Install the package and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if running in Colab)\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists('SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection'):\n",
    "        !git clone https://github.com/milinpatel07/SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection.git\n",
    "    os.chdir('SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection')\n",
    "    !pip install -q -e .\n",
    "else:\n",
    "    # Running locally: ensure you're in the repo root\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate / Load Ensemble Data\n",
    "\n",
    "The demo uses synthetic data that matches the statistical properties reported in the paper:\n",
    "- **465 proposals**: 135 true positives (TP), 330 false positives (FP)\n",
    "- **6 ensemble members** (SECOND detectors with different random seeds)\n",
    "- **22 environmental configurations** across 101 frames\n",
    "- **4 triggering condition categories**: heavy rain, night, fog/visibility, other\n",
    "\n",
    "To use real ensemble outputs, replace this cell with code that loads your prediction files (see Section 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.demo_data import generate_demo_dataset, get_individual_member_aps\n",
    "\n",
    "# Generate synthetic ensemble data\n",
    "data = generate_demo_dataset(\n",
    "    n_tp=135,\n",
    "    n_fp=330,\n",
    "    K=6,\n",
    "    n_frames=101,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "scores = data['scores']           # (465, 6) confidence scores per member\n",
    "boxes = data['boxes']             # (465, 6, 7) bounding boxes per member\n",
    "labels = data['labels']           # (465,) correctness: 1=TP, 0=FP\n",
    "frame_ids = data['frame_ids']     # (465,) frame assignment\n",
    "conditions = data['conditions']   # (465,) triggering condition category\n",
    "\n",
    "print(f\"Dataset: {len(labels)} proposals ({np.sum(labels==1)} TP, {np.sum(labels==0)} FP)\")\n",
    "print(f\"Ensemble size: K={data['K']}\")\n",
    "print(f\"Frames: {data['n_frames']}\")\n",
    "print(f\"Conditions: {np.unique(conditions)}\")\n",
    "\n",
    "# Individual member performance (Table 1)\n",
    "member_aps = get_individual_member_aps()\n",
    "print(f\"\\nIndividual Member BEV AP (IoU >= 0.5):\")\n",
    "for name, ap in member_aps.items():\n",
    "    print(f\"  Member {name}: {ap:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stage 2: Compute Uncertainty Indicators\n",
    "\n",
    "Three uncertainty indicators per proposal (Section 3.3, Table 2):\n",
    "\n",
    "| Indicator | Uncertainty Type | Safety Concern | SOTIF Activity |\n",
    "|-----------|-----------------|----------------|----------------|\n",
    "| Mean confidence $\\bar{s}_j$ | Existence | False/missed detection | PI identification (Cl. 7) |\n",
    "| Confidence variance $\\sigma^2_{s,j}$ | Epistemic | Unknown operating condition | TC identification (Cl. 7) |\n",
    "| Geometric disagreement $d_{\\text{iou},j}$ | Localisation | Incorrect distance estimate | PI characterisation (Cl. 7) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.uncertainty import compute_all_indicators\n",
    "\n",
    "# Compute all three uncertainty indicators\n",
    "indicators = compute_all_indicators(scores, boxes)\n",
    "\n",
    "mean_conf = indicators['mean_confidence']       # Eq. 1\n",
    "conf_var = indicators['confidence_variance']     # Eq. 2\n",
    "geo_disagree = indicators['geometric_disagreement']  # Eq. 3\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"UNCERTAINTY INDICATOR SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for name, vals, lab in [\n",
    "    (\"Mean Confidence\", mean_conf, labels),\n",
    "    (\"Confidence Variance\", conf_var, labels),\n",
    "    (\"Geometric Disagreement\", geo_disagree, labels),\n",
    "]:\n",
    "    tp_vals = vals[lab == 1]\n",
    "    fp_vals = vals[lab == 0]\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  TP: mean={np.mean(tp_vals):.4f}, median={np.median(tp_vals):.4f}, \"\n",
    "          f\"80th pct={np.percentile(tp_vals, 80):.5f}\")\n",
    "    print(f\"  FP: mean={np.mean(fp_vals):.4f}, median={np.median(fp_vals):.4f}, \"\n",
    "          f\"80th pct={np.percentile(fp_vals, 80):.5f}\")\n",
    "    print(f\"  Separation factor (80th pct FP / 80th pct TP): \"\n",
    "          f\"{np.percentile(fp_vals, 80) / max(np.percentile(tp_vals, 80), 1e-10):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 4: Discrimination Metrics\n",
    "\n",
    "**Key question**: *Does uncertainty identify which detections are incorrect?*\n",
    "\n",
    "This is the prerequisite for all SOTIF uses of uncertainty. If discrimination is poor, the subsequent analysis is not supported.\n",
    "\n",
    "- **AUROC** measures ranking quality (1.0 = perfect, 0.5 = random)\n",
    "- **AURC** measures residual risk as a function of coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.metrics import (\n",
    "    compute_auroc, compute_auroc_with_curve, compute_aurc, compute_all_metrics\n",
    ")\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(mean_conf, conf_var, geo_disagree, labels)\n",
    "\n",
    "disc = metrics['discrimination']\n",
    "print(\"=\" * 60)\n",
    "print(\"DISCRIMINATION: AUROC for TP vs. FP Separation (Table 3)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Mean Confidence (s_bar):          AUROC = {disc['auroc_mean_confidence']:.3f}\")\n",
    "print(f\"  Confidence Variance (sigma^2_s):   AUROC = {disc['auroc_confidence_variance']:.3f}\")\n",
    "print(f\"  Geometric Disagreement (d_iou):    AUROC = {disc['auroc_geometric_disagreement']:.3f}\")\n",
    "\n",
    "print(f\"\\n  Interpretation:\")\n",
    "print(f\"  - Mean confidence achieves near-perfect discrimination\")\n",
    "print(f\"  - Variance captures epistemic uncertainty (model disagreement)\")\n",
    "print(f\"  - Geometric disagreement captures localisation uncertainty\")\n",
    "print(f\"  - The three indicators are complementary (Table 2 in paper)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all three indicators\n",
    "from sotif_uncertainty.visualization import plot_roc_curves\n",
    "\n",
    "# Get ROC curve data for each indicator\n",
    "auroc_conf, fpr_conf, tpr_conf = compute_auroc_with_curve(\n",
    "    mean_conf, labels, higher_is_correct=True\n",
    ")\n",
    "auroc_var, fpr_var, tpr_var = compute_auroc_with_curve(\n",
    "    -conf_var, labels, higher_is_correct=True  # negate: lower var = correct\n",
    ")\n",
    "auroc_geo, fpr_geo, tpr_geo = compute_auroc_with_curve(\n",
    "    -geo_disagree, labels, higher_is_correct=True  # negate: lower disagreement = correct\n",
    ")\n",
    "\n",
    "roc_data = {\n",
    "    'Mean Confidence': (fpr_conf, tpr_conf, auroc_conf),\n",
    "    'Confidence Variance': (fpr_var, tpr_var, auroc_var),\n",
    "    'Geometric Disagreement': (fpr_geo, tpr_geo, auroc_geo),\n",
    "}\n",
    "\n",
    "fig = plot_roc_curves(roc_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 4: Calibration Metrics\n",
    "\n",
    "**Key question**: *Do confidence values match observed accuracy?*\n",
    "\n",
    "- **ECE**: Expected Calibration Error (Eq. 4)\n",
    "- **NLL**: Negative Log-Likelihood (Eq. 5)\n",
    "- **Brier Score**: Mean squared error between confidence and label (Eq. 6)\n",
    "\n",
    "**Important**: High discrimination and moderate calibration can coexist (Section 6.2). Acceptance gates depend on *ranking*, not absolute calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = metrics['calibration']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CALIBRATION AND SCORING METRICS (Table 4)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  ECE:         {cal['ece']:.3f}\")\n",
    "print(f\"  NLL:         {cal['nll']:.3f}\")\n",
    "print(f\"  Brier Score: {cal['brier']:.3f}\")\n",
    "print(f\"  AURC:        {metrics['risk_coverage']['aurc']:.3f}\")\n",
    "\n",
    "print(f\"\\n  Interpretation (Section 6.2):\")\n",
    "print(f\"  - ECE of {cal['ece']:.3f} means confidence deviates from accuracy by ~{cal['ece']*100:.0f} pp\")\n",
    "print(f\"  - Acceptance gates depend on RANKING (AUROC), not calibration (ECE)\")\n",
    "print(f\"  - If downstream modules interpret confidence probabilistically,\")\n",
    "print(f\"    post-hoc calibration (temperature scaling) can be applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability Diagram (Figure 5)\n",
    "from sotif_uncertainty.visualization import plot_reliability_diagram\n",
    "\n",
    "fig = plot_reliability_diagram(\n",
    "    cal['bin_accuracies'],\n",
    "    cal['bin_confidences'],\n",
    "    cal['bin_counts'],\n",
    "    cal['ece'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk-Coverage Curve (Figure 6)\n",
    "from sotif_uncertainty.visualization import plot_risk_coverage\n",
    "\n",
    "rc = metrics['risk_coverage']\n",
    "fig = plot_risk_coverage(rc['coverages'], rc['risks'], rc['aurc'])\n",
    "plt.show()\n",
    "\n",
    "# Find zero-risk coverage\n",
    "zero_risk_idx = 0\n",
    "for i, r in enumerate(rc['risks']):\n",
    "    if r > 0:\n",
    "        break\n",
    "    zero_risk_idx = i\n",
    "print(f\"Zero-risk region: coverage up to {rc['coverages'][zero_risk_idx]:.3f}\")\n",
    "print(f\"(i.e., the top {rc['coverages'][zero_risk_idx]*100:.1f}% most confident detections are all correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Uncertainty Landscape Visualisation\n",
    "\n",
    "Scatter plot of mean confidence vs. confidence variance, coloured by correctness label (Figure 7).\n",
    "\n",
    "**Expected pattern**: TP cluster at high confidence / low variance. FP spread across low confidence / high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_scatter_confidence_variance\n",
    "\n",
    "fig = plot_scatter_confidence_variance(mean_conf, conf_var, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 4: Operating Points (Acceptance Gates)\n",
    "\n",
    "**Key question**: *What is the tradeoff between retained capability and retained error?*\n",
    "\n",
    "An acceptance gate (Eq. 7) filters detections:\n",
    "\n",
    "$$G(\\bar{s}_j, \\sigma^2_{s,j}, d_{\\text{iou},j}) = [\\bar{s}_j \\geq \\tau_s] \\wedge [\\sigma^2_{s,j} \\leq \\tau_v] \\wedge [d_{\\text{iou},j} \\leq \\tau_d]$$\n",
    "\n",
    "ISO 21448, Clause 11 requires documented acceptance criteria. The operating point table provides this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import compute_operating_points, find_optimal_gate\n",
    "\n",
    "# Compute operating points with confidence-only and multi-indicator gates\n",
    "# Confidence-only gates\n",
    "conf_only_points = compute_operating_points(\n",
    "    mean_conf, conf_var, geo_disagree, labels,\n",
    "    tau_s_range=np.array([0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]),\n",
    ")\n",
    "\n",
    "# Multi-indicator gates\n",
    "multi_points = compute_operating_points(\n",
    "    mean_conf, conf_var, geo_disagree, labels,\n",
    "    tau_s_range=np.array([0.60, 0.65, 0.70]),\n",
    "    tau_v_range=np.array([0.002, 0.003]),\n",
    ")\n",
    "\n",
    "all_points = conf_only_points + multi_points\n",
    "\n",
    "# Display operating point table (Table 6)\n",
    "print(\"=\" * 75)\n",
    "print(\"OPERATING POINTS TABLE (Table 6 / Clause 11 Input)\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Gate':<35} {'Cov.':>6} {'Ret.':>5} {'FP':>4} {'FAR':>8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Show key operating points\n",
    "key_gates = [\n",
    "    \"s>=0.60\", \"s>=0.65\", \"s>=0.70\", \"s>=0.85\",\n",
    "    \"s>=0.65 & var<=0.0020\", \"s>=0.60 & var<=0.0020\",\n",
    "]\n",
    "for point in all_points:\n",
    "    if point['gate'] in key_gates or point['coverage'] > 0:\n",
    "        print(f\"{point['gate']:<35} {point['coverage']:>6.3f} {point['retained']:>5d} \"\n",
    "              f\"{point['fp']:>4d} {point['far']:>8.3f}\")\n",
    "\n",
    "# Find optimal gate for zero FAR\n",
    "print(\"\\n\" + \"-\" * 75)\n",
    "optimal = find_optimal_gate(\n",
    "    mean_conf, conf_var, geo_disagree, labels,\n",
    "    alpha=0.0,  # Zero false acceptance\n",
    "    tau_s_range=np.arange(0.50, 0.91, 0.05),\n",
    "    tau_v_range=np.array([np.inf, 0.002, 0.003]),\n",
    ")\n",
    "if optimal:\n",
    "    print(f\"Optimal gate (FAR=0): {optimal['gate']}\")\n",
    "    print(f\"  Coverage: {optimal['coverage']:.3f} ({optimal['retained']} detections retained)\")\n",
    "    print(f\"  All retained detections are correct (FAR=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating points visualisation\n",
    "from sotif_uncertainty.visualization import plot_operating_points_comparison\n",
    "\n",
    "fig = plot_operating_points_comparison(all_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Value of Multi-Indicator Gates (Section 6.3)\n",
    "\n",
    "A detection with $\\bar{s}=0.67$ and $\\sigma^2_s=0.001$ indicates member **agreement** (likely correct).  \n",
    "A detection with $\\bar{s}=0.67$ and $\\sigma^2_s=0.005$ indicates member **disagreement** (likely incorrect).\n",
    "\n",
    "The multi-indicator gate retains the first and rejects the second; a confidence-only gate at 0.65 retains both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import acceptance_gate, compute_coverage_far\n",
    "\n",
    "# Compare: confidence-only at 0.65 vs multi-indicator at 0.65+var<=0.002\n",
    "accepted_conf = acceptance_gate(mean_conf, conf_var, geo_disagree, tau_s=0.65)\n",
    "accepted_multi = acceptance_gate(mean_conf, conf_var, geo_disagree, tau_s=0.65, tau_v=0.002)\n",
    "\n",
    "cov_c, far_c, ret_c, fp_c = compute_coverage_far(accepted_conf, labels)\n",
    "cov_m, far_m, ret_m, fp_m = compute_coverage_far(accepted_multi, labels)\n",
    "\n",
    "print(\"Multi-indicator gate comparison:\")\n",
    "print(f\"  Confidence-only (s>=0.65):         {ret_c} retained, {fp_c} FP, FAR={far_c:.3f}\")\n",
    "print(f\"  Multi-indicator (s>=0.65, v<=0.002): {ret_m} retained, {fp_m} FP, FAR={far_m:.3f}\")\n",
    "print(f\"  Variance filter removes {ret_c - ret_m} detections, reducing FP by {fp_c - fp_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 5: SOTIF Analysis Artefacts\n",
    "\n",
    "### 7.1 Triggering Condition Identification (Clause 7)\n",
    "\n",
    "Rank conditions by FP share and mean uncertainty. If both rankings agree, uncertainty analysis independently identifies the triggering conditions that error analysis identifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import rank_triggering_conditions\n",
    "\n",
    "tc_results = rank_triggering_conditions(conditions, labels, mean_conf, conf_var)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRIGGERING CONDITION RANKING (Table 7 / Clause 7)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Condition':<18} {'FP Share':>10} {'Mean s (FP)':>12} {'Mean var (FP)':>14} {'TP':>5} {'FP':>5}\")\n",
    "print(\"-\" * 70)\n",
    "for tc in tc_results:\n",
    "    print(f\"{tc['condition']:<18} {tc['fp_share']:>10.1%} {tc['mean_conf_fp']:>12.2f} \"\n",
    "          f\"{tc['mean_var_fp']:>14.4f} {tc['tp_count']:>5d} {tc['fp_count']:>5d}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Heavy rain and night account for ~67% of all false positives\")\n",
    "print(f\"  - Ranking by FP share matches ranking by mean FP variance\")\n",
    "print(f\"  - This means uncertainty statistics alone can identify which\")\n",
    "print(f\"    conditions are most problematic, even without ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triggering condition visualisation\n",
    "from sotif_uncertainty.visualization import plot_tc_ranking\n",
    "\n",
    "fig = plot_tc_ranking(tc_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Frame-Level Triage (Clause 7)\n",
    "\n",
    "Flag frames containing high-uncertainty false positives for targeted investigation. This supports the Area 3 -> Area 2 transition (Figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import flag_frames, compute_frame_summary\n",
    "\n",
    "# Flag frames with high-uncertainty FP\n",
    "frame_flag_results = flag_frames(frame_ids, labels, conf_var, percentile=80.0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FRAME-LEVEL TRIAGE (Clause 7)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Variance threshold (80th pct of FP dist): {frame_flag_results['threshold']:.5f}\")\n",
    "print(f\"  Total frames: {frame_flag_results['total_frames']}\")\n",
    "print(f\"  Flagged frames: {frame_flag_results['flagged_count']}\")\n",
    "print(f\"  Flagged rate: {frame_flag_results['flagged_count']/frame_flag_results['total_frames']:.1%}\")\n",
    "print(f\"\\n  First 10 flagged frame IDs: {frame_flag_results['flagged_frames'][:10]}\")\n",
    "print(f\"\\n  These frames are candidates for targeted investigation,\")\n",
    "print(f\"  augmentation, or operational restriction.\")\n",
    "\n",
    "# Compute per-frame summaries\n",
    "frame_summaries = compute_frame_summary(frame_ids, labels, mean_conf, conf_var, conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame risk scatter (Figure 8)\n",
    "from sotif_uncertainty.visualization import plot_frame_risk_scatter\n",
    "\n",
    "fig = plot_frame_risk_scatter(frame_summaries)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 ISO 21448 Scenario Categorisation\n",
    "\n",
    "The SOTIF objective is to reduce the space of unknown hazardous scenarios (Area 3) by identifying triggering conditions and moving them to Area 2 where mitigation measures can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_iso21448_scenario_grid\n",
    "\n",
    "fig = plot_iso21448_scenario_grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete SOTIF Artefact Summary\n",
    "\n",
    "Summary of all artefacts produced, mapped to ISO 21448 clauses (Table 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 75)\n",
    "print(\"SOTIF ARTEFACT SUMMARY (Table 5)\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "artefacts = [\n",
    "    (\"Per-detection AUROC, AURC\",\n",
    "     \"Performance insufficiency identification (Clause 7)\",\n",
    "     f\"AUROC(conf)={disc['auroc_mean_confidence']:.3f}, \"\n",
    "     f\"AUROC(var)={disc['auroc_confidence_variance']:.3f}\"),\n",
    "    (\"Per-condition FP rate & ranking\",\n",
    "     \"Triggering condition identification (Clause 7)\",\n",
    "     f\"Top TCs: {tc_results[0]['condition']} ({tc_results[0]['fp_share']:.0%}), \"\n",
    "     f\"{tc_results[1]['condition']} ({tc_results[1]['fp_share']:.0%})\"),\n",
    "    (\"Frame flags (high-var FP)\",\n",
    "     \"Scenario triage: Area 3 -> Area 2 (Clause 7)\",\n",
    "     f\"{frame_flag_results['flagged_count']}/{frame_flag_results['total_frames']} frames flagged\"),\n",
    "    (\"Coverage & FAR at thresholds\",\n",
    "     \"Acceptance criteria input (Clause 11)\",\n",
    "     f\"At s>=0.70: cov={[p for p in conf_only_points if abs(p['tau_s']-0.70)<0.01][0]['coverage']:.3f}, FAR={[p for p in conf_only_points if abs(p['tau_s']-0.70)<0.01][0]['far']:.3f}\" if any(abs(p['tau_s']-0.70)<0.01 for p in conf_only_points) else \"N/A\"),\n",
    "    (\"ECE, NLL, Brier\",\n",
    "     \"Confidence interpretation (Clause 10)\",\n",
    "     f\"ECE={cal['ece']:.3f}, NLL={cal['nll']:.3f}, Brier={cal['brier']:.3f}\"),\n",
    "]\n",
    "\n",
    "for output, activity, value in artefacts:\n",
    "    print(f\"\\n  Output: {output}\")\n",
    "    print(f\"  SOTIF Activity: {activity}\")\n",
    "    print(f\"  Result: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Real Data\n",
    "\n",
    "To run this evaluation on real ensemble outputs:\n",
    "\n",
    "### Step 1: Train Ensemble (requires GPU)\n",
    "```bash\n",
    "# Train 6 SECOND models with different random seeds using OpenPCDet\n",
    "bash scripts/train_ensemble.sh --seeds 0 1 2 3 4 5\n",
    "```\n",
    "\n",
    "### Step 2: Run Ensemble Inference\n",
    "```bash\n",
    "python scripts/run_inference.py \\\n",
    "    --cfg_file tools/cfgs/kitti_models/second.yaml \\\n",
    "    --ckpt_dirs output/seed_0 output/seed_1 ... output/seed_5 \\\n",
    "    --data_path data/kitti/testing\n",
    "```\n",
    "\n",
    "### Step 3: Load Real Data Instead of Synthetic\n",
    "```python\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load ensemble predictions (one pickle per member)\n",
    "member_preds = []\n",
    "for seed in range(6):\n",
    "    with open(f'output/seed_{seed}/result.pkl', 'rb') as f:\n",
    "        member_preds.append(pickle.load(f))\n",
    "\n",
    "# Format into scores array (N, K) and boxes array (N, K, 7)\n",
    "# Then run the same pipeline as above\n",
    "```\n",
    "\n",
    "### Available Datasets\n",
    "\n",
    "| Dataset | Size | Format | Weather | Access |\n",
    "|---------|------|--------|---------|--------|\n",
    "| KITTI | ~29 GB | Native | Clear only | Free (cvlibs.net) |\n",
    "| nuScenes mini | ~4 GB | Convertible | Rain/night | Free (nuscenes.org) |\n",
    "| MultiFog KITTI | Varies | KITTI | Synthetic fog | Free |\n",
    "| CADC | ~7K frames | Custom | Snow/winter | Free |\n",
    "| CARLA (generate) | Custom | KITTI | Any | Free (simulator) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate All Figures\n",
    "\n",
    "Save all figures to the `Analysis/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import generate_all_figures\n",
    "\n",
    "figures = generate_all_figures(\n",
    "    metrics=metrics,\n",
    "    mean_conf=mean_conf,\n",
    "    conf_var=conf_var,\n",
    "    labels=labels,\n",
    "    frame_summaries=frame_summaries,\n",
    "    tc_results=tc_results,\n",
    "    operating_points=all_points,\n",
    "    output_dir='Analysis',\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(figures)} figures:\")\n",
    "for name in figures:\n",
    "    print(f\"  - Analysis/{name}*.png\")\n",
    "print(\"\\nAll figures saved to Analysis/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "name": "SOTIF_Uncertainty_Evaluation_Demo.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
