{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTIF Uncertainty Evaluation for LiDAR Object Detection\n",
    "\n",
    "**Uncertainty Evaluation to Support Safety of the Intended Functionality Analysis for Identifying Performance Insufficiencies in ML-Based LiDAR Object Detection**\n",
    "\n",
    "*Milin Patel and Rolf Jung, Kempten University of Applied Sciences*\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook implements the complete **five-stage evaluation methodology** from the paper and produces all **four SOTIF artefacts** mapped to ISO 21448 clauses:\n",
    "\n",
    "| Stage | Description | Section |\n",
    "|-------|-------------|---------|\n",
    "| Stage 1 | Ensemble Inference (K=6 SECOND detectors) | 3.2 |\n",
    "| Stage 2 | Cross-Member Association & Uncertainty Indicators | 3.3 |\n",
    "| Stage 3 | Correctness Determination (greedy BEV IoU matching) | 3.4 |\n",
    "| Stage 4 | Metric Computation (discrimination, calibration, operating characteristics) | 3.5 |\n",
    "| Stage 5 | SOTIF Analysis (TC ranking, frame flags, acceptance gates) | 3.6 |\n",
    "\n",
    "| SOTIF Artefact | ISO 21448 Clause | What It Produces |\n",
    "|----------------|------------------|------------------|\n",
    "| Per-detection discrimination | Clause 7 | AUROC separating TP from FP |\n",
    "| Triggering condition ranking | Clause 7 | Conditions ranked by FP share and uncertainty |\n",
    "| Frame-level triage | Clause 7 | Flagged frames for investigation (Area 3 to Area 2) |\n",
    "| Acceptance criteria | Clause 11 | Operating points with coverage and FAR |\n",
    "\n",
    "**No GPU required.** Uses synthetic data matching the paper's statistics. For real data, see Section 10.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/milinpatel07/SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection/blob/main/notebooks/SOTIF_Uncertainty_Evaluation_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists('SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection'):\n",
    "        !git clone https://github.com/milinpatel07/SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection.git\n",
    "    os.chdir('SOTIF_Uncertainty_Evaluation_Lidar_Object_Detection')\n",
    "    !pip install -q -e .\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(f'NumPy {np.__version__} | Matplotlib {matplotlib.__version__}')\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Ensemble Data (Stage 1)\n",
    "\n",
    "The demo generates synthetic ensemble outputs matching the paper's case study:\n",
    "- **465 proposals**: 135 TP, 330 FP (BEV IoU >= 0.5)\n",
    "- **K=6** SECOND detectors trained with different random seeds\n",
    "- **22 environmental configurations** across 101 frames\n",
    "- **4 triggering condition categories**: heavy rain, night, fog/visibility, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.demo_data import generate_demo_dataset, get_individual_member_aps\n",
    "\n",
    "data = generate_demo_dataset(n_tp=135, n_fp=330, K=6, n_frames=101, seed=42)\n",
    "\n",
    "scores = data['scores']           # (465, 6)\n",
    "boxes = data['boxes']             # (465, 6, 7)\n",
    "labels = data['labels']           # (465,)\n",
    "frame_ids = data['frame_ids']     # (465,)\n",
    "conditions = data['conditions']   # (465,)\n",
    "\n",
    "print(f'Dataset Statistics (Section 5.1):')\n",
    "print(f'  Total proposals:  {len(labels)}')\n",
    "print(f'  True positives:   {np.sum(labels==1)}')\n",
    "print(f'  False positives:  {np.sum(labels==0)}')\n",
    "print(f'  FP:TP ratio:      {np.sum(labels==0)/np.sum(labels==1):.1f}:1')\n",
    "print(f'  Ensemble size:    K={data[\"K\"]}')\n",
    "print(f'  Frames:           {data[\"n_frames\"]}')\n",
    "print(f'  Conditions:       {sorted(np.unique(conditions))}')\n",
    "\n",
    "print(f'\\nIndividual Member BEV AP (Table 1):')\n",
    "for name, ap in get_individual_member_aps().items():\n",
    "    print(f'  Member {name}: {ap:.2f}%')\n",
    "print(f'  Range: {min(get_individual_member_aps().values()):.2f}% - {max(get_individual_member_aps().values()):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stage 2: Uncertainty Indicators\n",
    "\n",
    "Three uncertainty indicators per proposal, each capturing a distinct aspect of detection unreliability:\n",
    "\n",
    "**Mean confidence** $\\bar{s}_j = \\frac{1}{K} \\sum_{k=1}^{K} s_j^{(k)}$ -- existence uncertainty (Eq. 1)\n",
    "\n",
    "**Confidence variance** $\\sigma^2_{s,j} = \\frac{1}{K-1} \\sum_{k=1}^{K} (s_j^{(k)} - \\bar{s}_j)^2$ -- epistemic uncertainty (Eq. 2)\n",
    "\n",
    "**Geometric disagreement** $d_{\\text{iou},j} = 1 - \\frac{2}{K(K-1)} \\sum_{u<v} \\text{IoU}_{\\text{BEV}}(b_j^{(u)}, b_j^{(v)})$ -- localisation uncertainty (Eq. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.uncertainty import compute_all_indicators\n",
    "\n",
    "indicators = compute_all_indicators(scores, boxes)\n",
    "mean_conf = indicators['mean_confidence']\n",
    "conf_var = indicators['confidence_variance']\n",
    "geo_disagree = indicators['geometric_disagreement']\n",
    "\n",
    "tp = labels == 1\n",
    "fp = labels == 0\n",
    "\n",
    "print('Uncertainty Indicator Summary')\n",
    "print('=' * 65)\n",
    "print(f'{\"Indicator\":<25} {\"TP mean\":>10} {\"FP mean\":>10} {\"TP 80th\":>10} {\"FP 80th\":>10}')\n",
    "print('-' * 65)\n",
    "for name, vals in [('Mean confidence', mean_conf), ('Confidence variance', conf_var), ('Geo. disagreement', geo_disagree)]:\n",
    "    print(f'{name:<25} {np.mean(vals[tp]):>10.4f} {np.mean(vals[fp]):>10.4f} '\n",
    "          f'{np.percentile(vals[tp], 80):>10.5f} {np.percentile(vals[fp], 80):>10.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Indicator Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_indicator_distributions\n",
    "fig = plot_indicator_distributions(mean_conf, conf_var, geo_disagree, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Uncertainty Landscape (Figure 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_scatter_confidence_variance\n",
    "fig = plot_scatter_confidence_variance(mean_conf, conf_var, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ensemble Member Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_member_agreement\n",
    "fig = plot_member_agreement(scores, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 4: Discrimination Metrics (Table 3)\n",
    "\n",
    "**Key question**: Does uncertainty identify which detections are incorrect?\n",
    "\n",
    "AUROC measures ranking quality: 1.0 = perfect separation, 0.5 = random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.metrics import compute_all_metrics, compute_auroc_with_curve\n",
    "\n",
    "metrics = compute_all_metrics(mean_conf, conf_var, geo_disagree, labels)\n",
    "disc = metrics['discrimination']\n",
    "\n",
    "print('Discrimination: AUROC for TP vs. FP Separation (Table 3)')\n",
    "print('=' * 55)\n",
    "print(f'  Mean Confidence (s_bar):          AUROC = {disc[\"auroc_mean_confidence\"]:.3f}')\n",
    "print(f'  Confidence Variance (sigma^2_s):   AUROC = {disc[\"auroc_confidence_variance\"]:.3f}')\n",
    "print(f'  Geometric Disagreement (d_iou):    AUROC = {disc[\"auroc_geometric_disagreement\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_roc_curves\n",
    "\n",
    "auroc_conf, fpr_conf, tpr_conf = compute_auroc_with_curve(mean_conf, labels, True)\n",
    "auroc_var, fpr_var, tpr_var = compute_auroc_with_curve(-conf_var, labels, True)\n",
    "auroc_geo, fpr_geo, tpr_geo = compute_auroc_with_curve(-geo_disagree, labels, True)\n",
    "\n",
    "fig = plot_roc_curves({\n",
    "    'Mean Confidence': (fpr_conf, tpr_conf, auroc_conf),\n",
    "    'Confidence Variance': (fpr_var, tpr_var, auroc_var),\n",
    "    'Geometric Disagreement': (fpr_geo, tpr_geo, auroc_geo),\n",
    "})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 4: Calibration Metrics (Table 4)\n",
    "\n",
    "**Key question**: Do confidence values match observed accuracy?\n",
    "\n",
    "High discrimination (AUROC~1.0) and moderate calibration (ECE~0.2) coexist because they measure different properties. Acceptance gates depend on ranking, not absolute calibration (Section 6.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = metrics['calibration']\n",
    "rc = metrics['risk_coverage']\n",
    "\n",
    "print('Calibration and Scoring Metrics (Table 4)')\n",
    "print('=' * 40)\n",
    "print(f'  ECE:         {cal[\"ece\"]:.3f}')\n",
    "print(f'  NLL:         {cal[\"nll\"]:.3f}')\n",
    "print(f'  Brier Score: {cal[\"brier\"]:.3f}')\n",
    "print(f'  AURC:        {rc[\"aurc\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_reliability_diagram, plot_risk_coverage\n",
    "\n",
    "fig = plot_reliability_diagram(cal['bin_accuracies'], cal['bin_confidences'], cal['bin_counts'], cal['ece'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_risk_coverage(rc['coverages'], rc['risks'], rc['aurc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 4: Operating Points / Acceptance Gates (Table 6)\n",
    "\n",
    "The acceptance gate (Eq. 7) filters detections deterministically:\n",
    "\n",
    "$$G(\\bar{s}_j, \\sigma^2_{s,j}, d_{\\text{iou},j}) = [\\bar{s}_j \\geq \\tau_s] \\wedge [\\sigma^2_{s,j} \\leq \\tau_v] \\wedge [d_{\\text{iou},j} \\leq \\tau_d]$$\n",
    "\n",
    "ISO 21448 Clause 11 requires documented acceptance criteria. The operating point table provides this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import compute_operating_points, find_optimal_gate\n",
    "\n",
    "conf_only = compute_operating_points(\n",
    "    mean_conf, conf_var, geo_disagree, labels,\n",
    "    tau_s_range=np.array([0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]),\n",
    ")\n",
    "multi = compute_operating_points(\n",
    "    mean_conf, conf_var, geo_disagree, labels,\n",
    "    tau_s_range=np.array([0.60, 0.65, 0.70]),\n",
    "    tau_v_range=np.array([0.002, 0.003]),\n",
    ")\n",
    "all_points = conf_only + multi\n",
    "\n",
    "print('Operating Points Table (Table 6 / Clause 11)')\n",
    "print('=' * 70)\n",
    "print(f'{\"Gate\":<35} {\"Cov.\":>6} {\"Ret.\":>5} {\"FP\":>4} {\"FAR\":>8}')\n",
    "print('-' * 70)\n",
    "for p in all_points:\n",
    "    if p['coverage'] > 0:\n",
    "        print(f'{p[\"gate\"]:<35} {p[\"coverage\"]:>6.3f} {p[\"retained\"]:>5d} {p[\"fp\"]:>4d} {p[\"far\"]:>8.3f}')\n",
    "\n",
    "optimal = find_optimal_gate(\n",
    "    mean_conf, conf_var, geo_disagree, labels, alpha=0.0,\n",
    "    tau_s_range=np.arange(0.50, 0.91, 0.05),\n",
    "    tau_v_range=np.array([np.inf, 0.002, 0.003]),\n",
    ")\n",
    "if optimal:\n",
    "    print(f'\\nOptimal zero-FAR gate: {optimal[\"gate\"]}')\n",
    "    print(f'  Coverage: {optimal[\"coverage\"]:.3f} ({optimal[\"retained\"]} detections, all correct)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_operating_points_comparison, plot_operating_point_heatmap\n",
    "\n",
    "fig = plot_operating_points_comparison(all_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_operating_point_heatmap(mean_conf, conf_var, geo_disagree, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 5: Triggering Condition Identification (Clause 7, Table 7)\n",
    "\n",
    "Rank conditions by FP share and mean uncertainty. If both rankings agree, uncertainty analysis independently identifies the triggering conditions that error analysis identifies -- even without ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import rank_triggering_conditions\n",
    "\n",
    "tc_results = rank_triggering_conditions(conditions, labels, mean_conf, conf_var)\n",
    "\n",
    "print('Triggering Condition Ranking (Table 7 / Clause 7)')\n",
    "print('=' * 75)\n",
    "print(f'{\"Condition\":<18} {\"FP Share\":>10} {\"Mean s (FP)\":>12} {\"Mean var (FP)\":>14} {\"TP\":>5} {\"FP\":>5}')\n",
    "print('-' * 75)\n",
    "for tc in tc_results:\n",
    "    print(f'{tc[\"condition\"]:<18} {tc[\"fp_share\"]:>10.1%} {tc[\"mean_conf_fp\"]:>12.3f} '\n",
    "          f'{tc[\"mean_var_fp\"]:>14.5f} {tc[\"tp_count\"]:>5d} {tc[\"fp_count\"]:>5d}')\n",
    "\n",
    "# Verify ranking agreement\n",
    "by_share = [tc['condition'] for tc in tc_results]\n",
    "by_var = [tc['condition'] for tc in sorted(tc_results, key=lambda x: x['mean_var_fp'], reverse=True)]\n",
    "print(f'\\nRanking by FP share:    {by_share}')\n",
    "print(f'Ranking by mean FP var: {by_var}')\n",
    "print(f'Rankings match: {by_share == by_var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_tc_ranking, plot_condition_breakdown, plot_condition_boxplots\n",
    "\n",
    "fig = plot_tc_ranking(tc_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_condition_breakdown(labels, conditions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_condition_boxplots(mean_conf, conf_var, labels, conditions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 5: Frame-Level Triage (Clause 7)\n",
    "\n",
    "Flag frames containing high-uncertainty false positives for targeted investigation.\n",
    "Supports the Area 3 -> Area 2 transition in ISO 21448."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.sotif_analysis import flag_frames, compute_frame_summary\n",
    "from sotif_uncertainty.visualization import plot_frame_risk_scatter\n",
    "\n",
    "flag_results = flag_frames(frame_ids, labels, conf_var, percentile=80.0)\n",
    "frame_summaries = compute_frame_summary(frame_ids, labels, mean_conf, conf_var, conditions)\n",
    "\n",
    "print('Frame-Level Triage (Clause 7)')\n",
    "print('=' * 50)\n",
    "print(f'  Variance threshold (80th pct FP): {flag_results[\"threshold\"]:.5f}')\n",
    "print(f'  Total frames:  {flag_results[\"total_frames\"]}')\n",
    "print(f'  Flagged frames: {flag_results[\"flagged_count\"]}')\n",
    "\n",
    "fig = plot_frame_risk_scatter(frame_summaries)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ISO 21448 Scenario Categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_iso21448_scenario_grid\n",
    "fig = plot_iso21448_scenario_grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import plot_summary_dashboard\n",
    "fig = plot_summary_dashboard(metrics, mean_conf, conf_var, labels, tc_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete SOTIF Artefact Summary (Table 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SOTIF Artefact Summary (Table 5)')\n",
    "print('=' * 80)\n",
    "\n",
    "artefacts = [\n",
    "    ('Per-detection AUROC, AURC',\n",
    "     'Performance insufficiency identification (Clause 7)',\n",
    "     f'AUROC(conf)={disc[\"auroc_mean_confidence\"]:.3f}, AUROC(var)={disc[\"auroc_confidence_variance\"]:.3f}'),\n",
    "    ('Per-condition FP rate & uncertainty ranking',\n",
    "     'Triggering condition identification (Clause 7)',\n",
    "     f'{tc_results[0][\"condition\"]} ({tc_results[0][\"fp_share\"]:.0%}), '\n",
    "     f'{tc_results[1][\"condition\"]} ({tc_results[1][\"fp_share\"]:.0%})'),\n",
    "    ('Frame flags (high-variance FP)',\n",
    "     'Scenario triage: Area 3 -> Area 2 (Clause 7)',\n",
    "     f'{flag_results[\"flagged_count\"]}/{flag_results[\"total_frames\"]} frames flagged'),\n",
    "    ('Coverage & FAR at thresholds',\n",
    "     'Acceptance criteria input (Clause 11)',\n",
    "     f'At s>=0.70: cov={[p for p in conf_only if abs(p[\"tau_s\"]-0.70)<0.01][0][\"coverage\"]:.3f}, '\n",
    "     f'FAR={[p for p in conf_only if abs(p[\"tau_s\"]-0.70)<0.01][0][\"far\"]:.3f}' if any(abs(p['tau_s']-0.70)<0.01 for p in conf_only) else 'N/A'),\n",
    "    ('ECE, NLL, Brier',\n",
    "     'Confidence interpretation (Clause 10)',\n",
    "     f'ECE={cal[\"ece\"]:.3f}, NLL={cal[\"nll\"]:.3f}, Brier={cal[\"brier\"]:.3f}'),\n",
    "]\n",
    "\n",
    "for output, activity, value in artefacts:\n",
    "    print(f'\\n  Output:   {output}')\n",
    "    print(f'  Clause:   {activity}')\n",
    "    print(f'  Result:   {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save All Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sotif_uncertainty.visualization import generate_all_figures\n",
    "\n",
    "figures = generate_all_figures(\n",
    "    metrics=metrics,\n",
    "    mean_conf=mean_conf,\n",
    "    conf_var=conf_var,\n",
    "    labels=labels,\n",
    "    frame_summaries=frame_summaries,\n",
    "    tc_results=tc_results,\n",
    "    operating_points=all_points,\n",
    "    output_dir='Analysis',\n",
    "    scores=scores,\n",
    "    geo_disagree=geo_disagree,\n",
    "    conditions=conditions,\n",
    ")\n",
    "\n",
    "print(f'Generated {len(figures)} figures:')\n",
    "for name in sorted(figures.keys()):\n",
    "    print(f'  - Analysis/{name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Using Real Data\n",
    "\n",
    "To run this evaluation on real ensemble outputs:\n",
    "\n",
    "### Option A: KITTI Dataset\n",
    "```bash\n",
    "# 1. Download KITTI\n",
    "python scripts/prepare_kitti.py --data_root data/kitti\n",
    "\n",
    "# 2. Install OpenPCDet\n",
    "git clone https://github.com/open-mmlab/OpenPCDet.git\n",
    "cd OpenPCDet && pip install -e . && cd ..\n",
    "\n",
    "# 3. Train 6 SECOND models with different seeds\n",
    "bash scripts/train_ensemble.sh --seeds 0 1 2 3 4 5\n",
    "\n",
    "# 4. Run ensemble inference\n",
    "python scripts/run_inference.py --ckpt_dirs output/ensemble/seed_*\n",
    "\n",
    "# 5. Evaluate\n",
    "python scripts/evaluate.py --input results/ensemble_results.pkl\n",
    "```\n",
    "\n",
    "### Option B: CARLA Simulation (reproduces paper exactly)\n",
    "```bash\n",
    "# Generate KITTI-format data from CARLA with 22 weather configs\n",
    "# See: https://github.com/fnozarian/CARLA-KITTI\n",
    "```\n",
    "\n",
    "### Available Datasets\n",
    "\n",
    "| Dataset | Size | Format | Weather | Access |\n",
    "|---------|------|--------|---------|--------|\n",
    "| KITTI | ~29 GB | Native | Clear | Free (cvlibs.net) |\n",
    "| nuScenes mini | ~4 GB | Convertible | Rain, night | Free (nuscenes.org) |\n",
    "| MultiFog KITTI | Varies | KITTI | Synthetic fog | Free |\n",
    "| CADC | ~7K frames | Custom | Snow/winter | Free (cadcd.uwaterloo.ca) |\n",
    "| CARLA | Custom | KITTI | Any (simulated) | Free (carla.org) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "name": "SOTIF_Uncertainty_Evaluation_Demo.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
